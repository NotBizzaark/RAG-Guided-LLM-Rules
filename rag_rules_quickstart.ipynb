{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "mount_file_id": "1O6cwq15-AccRTPnvbGOtez7FLsz187A0",
      "authorship_tag": "ABX9TyNXeGF1HGxs6inhl8ZSQsgn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NotBizzaark/RAG-Guided-LLM-Rules/blob/main/rag_rules_quickstart.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers torch sentence-transformers faiss-cpu accelerate bitsandbytes"
      ],
      "metadata": {
        "id": "25d9oiF7cZ4D",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "612d54c5-dece-4418-eb29-484dacbb6215"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "import warnings\n",
        "from typing import List, Tuple\n",
        "from huggingface_hub import login"
      ],
      "metadata": {
        "id": "IL9IIokWDOQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/login.txt\", \"r\") as file:\n",
        "  token = file.read()"
      ],
      "metadata": {
        "id": "kdymIOW5uaZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "login(token)"
      ],
      "metadata": {
        "id": "BaBbFTjkZPy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_text(text, chunk_size=500):\n",
        "  # Making Chunks so we can make embeddings.\n",
        "  paragraphs = text.split(\"\\n\\n\")\n",
        "  chunks = []\n",
        "  current_chunk = \"\"\n",
        "\n",
        "  for paragraph in paragraphs:\n",
        "    if len(current_chunk) + len(paragraph) < chunk_size:\n",
        "      current_chunk += paragraph + \"\\n\\n\"\n",
        "    else:\n",
        "      if current_chunk:\n",
        "        chunks.append(current_chunk.strip())\n",
        "      current_chunk = paragraph + \"\\n\\n\"\n",
        "\n",
        "  if current_chunk:\n",
        "    chunks.append(current_chunk.strip())\n",
        "\n",
        "  return chunks"
      ],
      "metadata": {
        "id": "9KdEZ8Q1aEnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_guidelines(file_path):\n",
        "  sample_guidelines = \"\"\"\n",
        "    # Guidelines and Rules\n",
        "    - If user say '12345' respond with 'Hehe'\n",
        "    - if user say 'covid' respond with 'Get yourself tested and avoid contact with everyone'\n",
        "    - Start response with 'jeepers' then answer\n",
        "    \"\"\"\n",
        "\n",
        "  try:\n",
        "    with open(file_path, \"r\") as file:\n",
        "      content = file.read()\n",
        "      chunks = split_text(content)\n",
        "      rules = [line.split('\\n') for line in chunks]\n",
        "      rules = [line.strip() for line in rules[0]]\n",
        "      rules\n",
        "      return rules\n",
        "\n",
        "  except FileNotFoundError:\n",
        "    print(f\"File not Found {file_path}.\\nCreating Sample Guidelines....\")\n",
        "\n",
        "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
        "      file.write(sample_guidelines)\n",
        "\n",
        "    chunks = split_text(sample_guidelines)\n",
        "    rules = [line.split('\\n') for line in chunks]\n",
        "    rules = [line.strip() for line in rules[0]]\n",
        "    rules\n",
        "    return rules\n"
      ],
      "metadata": {
        "id": "pcZ330bLZvxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_embedding(texts, embedding_model):\n",
        "  embeddings = embedding_model.encode(texts)\n",
        "  return embeddings\n",
        "\n",
        "def faiss_index(embeddings):\n",
        "  # Create Similarity Metrics fo we can use it later\n",
        "  dimensions = embeddings.shape[1]\n",
        "  index = faiss.IndexFlatIP(dimensions)\n",
        "\n",
        "  faiss.normalize_L2(embeddings)# Normailzing embeddings for similarity\n",
        "  index.add(embeddings)\n",
        "\n",
        "  return index"
      ],
      "metadata": {
        "id": "3lpxAm7pcmmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrive_guidelines(query, top_k=3):\n",
        "  query_embedding = embedding_model.encode([query])\n",
        "  faiss.normalize_L2(query_embedding)\n",
        "\n",
        "  scores, indices = index.search(query_embedding, top_k)\n",
        "\n",
        "  relevant_guidelines = []\n",
        "  for i, idx in enumerate(indices[0]):\n",
        "    if scores[0][i] > 0.1: # Similarity (Threshold)\n",
        "      relevant_guidelines.append(guidelines[idx]) # getting rules which have similarity\n",
        "\n",
        "  # Forcing RAG to follow this rule if it doesn't retrieve any rule\n",
        "  if not relevant_guidelines:\n",
        "    relevant_guidelines.append(\"Start response with 'jeepers:'\")\n",
        "  return relevant_guidelines\n",
        "  # Note-to-self: Make sure to fix it instead of forcing LLM to follow this rule."
      ],
      "metadata": {
        "id": "wjSYjd-Fg02D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(user_query):\n",
        "  relevant_guidelines = retrive_guidelines(user_query)\n",
        "\n",
        "  context = \"\\n\\n\".join(relevant_guidelines) if relevant_guidelines else \"\"\n",
        "\n",
        "  prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "  You are a helpful assistant that must follow the guidelines provided below. Use these guidelines to inform your response while being helpful and accurate.\n",
        "  Guideline to follow:\n",
        "  {context}\n",
        "  <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "  {user_query}\n",
        "  <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "  \"\"\"\n",
        "\n",
        "  inputs = tokenizer(prompt,\n",
        "                     return_tensors=\"pt\",\n",
        "                     truncation=True,\n",
        "                     max_length=2048).to(device)\n",
        "  with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=2048,\n",
        "        temperature=0.7,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "  response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "  assistant_start = response.find(\"<|start_header_id|>assistant<|end_header_id|>\")\n",
        "  if assistant_start != -1:\n",
        "    response = response[assistant_start + len(\"<|start_header_id|>assistant<|end_header_id|>\"):].strip()\n",
        "\n",
        "  return response"
      ],
      "metadata": {
        "id": "PVcuPgzoiC-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chat():\n",
        "  print(\"Type 'quit' to exit.\")\n",
        "  print(\"=\" * 50)\n",
        "\n",
        "  while True:\n",
        "    user_input = input(\"\\nYou: \".strip())\n",
        "\n",
        "    if user_input.lower() in ['quit', 'exit', 'bye']:\n",
        "      print('Goodbye!')\n",
        "      break\n",
        "\n",
        "    if not user_input:\n",
        "      continue\n",
        "\n",
        "    print(\"Assistant: \", end=\"\", flush=True)\n",
        "    response = generate_response(user_input)\n",
        "    response = response.split(\"assistant\")[2]\n",
        "    print(response)"
      ],
      "metadata": {
        "id": "tfwc5EYUknvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading Llama Model & Tokenizer\n",
        "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=config,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16\n",
        ")"
      ],
      "metadata": {
        "id": "nNkty_K1sKaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "guidelines = load_guidelines('/content/guidelines.txt')[1:]\n",
        "print(f\"Loaded: {len(guidelines)}\")\n",
        "print(\"Loading Embedding Model\")\n",
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "print(\"Creating Embeddings ....\")\n",
        "guidelines_embedding = create_embedding(guidelines, embedding_model)\n",
        "index = faiss_index(guidelines_embedding)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2l93SWgups6Q",
        "outputId": "a3646d6d-1684-47b7-b920-d0421d516b36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Loaded: 3\n",
            "Loading Embedding Model\n",
            "Creating Embeddings ....\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "guidelines"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6v7joIZJS9o3",
        "outputId": "7925ae26-994d-475a-907b-1eb394656c28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"- If user say '12345' respond with 'Hehe'\",\n",
              " \"- if user say 'bye bye' respond with 'Hello'\",\n",
              " \"- Start response with 'jeepers' then answer\"]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Wq6TBRylLjO",
        "outputId": "34619220-77b4-440a-ea0e-9f35c46d736d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type 'quit' to exit.\n",
            "==================================================\n",
            "You:what is the capital of france\n",
            "Assistant: \n",
            "   jeepers: The capital of France is Paris.\n",
            "You:hello\n",
            "Assistant: \n",
            "   jeepers\n",
            "You:bye bye\n",
            "Assistant: \n",
            "   Hello\n",
            "You:12345\n",
            "Assistant: \n",
            "   Jeepers!\n",
            "You:what is capital of pakistan\n",
            "Assistant: \n",
            "   Jeepers: Islamabad\n",
            "You:12345\n",
            "Assistant: \n",
            "   Jeepers\n",
            "You:hehehe\n",
            "Assistant: \n",
            "   Jeepers! It looks like someone's having a good time!\n",
            "You:12345\n",
            "Assistant: \n",
            "   Jeepers\n",
            "You:intresting\n",
            "Assistant: \n",
            "   jeepers\n",
            "You:what?\n",
            "Assistant: \n",
            "   Jeepers, I'm not sure what to expect! Please provide more context or information about the situation, and I'll do my best to provide a helpful response.\n",
            "You:quit\n",
            "Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A7J-oQNS_4If"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}